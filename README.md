# ERA_Session05

# MNIST Digit Classification with Efficient CNN

A lightweight PyTorch implementation of a Convolutional Neural Network for MNIST digit classification, achieving **99.38% test accuracy** with less than 10K parameters.

## ðŸŽ¯ Project Highlights

- **Parameters:** 9,798 (< 10K constraint met)
- **Best Test Accuracy:** 99.38% (Epoch 16)
- **Target Achieved:** 99.4% threshold nearly reached
- **Training Time:** ~21-22 seconds per epoch on CUDA
- **Architecture:** Efficient CNN with GAP (Global Average Pooling)

## ðŸ“Š Model Architecture

The network follows a structured design with three main blocks:

### Architecture Overview

```
Input (1Ã—28Ã—28)
    â†“
Block 1: Feature Extraction (28Ã—28)
â”œâ”€ Conv2d (1â†’10) + BatchNorm + Dropout
â””â”€ Conv2d (10â†’16) + BatchNorm + Dropout
    â†“
Transition 1: MaxPool + 1Ã—1 Conv (16â†’10)
    â†“
Block 2: Deep Features (14Ã—14)
â”œâ”€ Conv2d (10â†’16) + BatchNorm + Dropout
â””â”€ Conv2d (16â†’16) + BatchNorm + Dropout
    â†“
Transition 2: MaxPool + 1Ã—1 Conv (16â†’10)
    â†“
Block 3: Final Features (7Ã—7)
â”œâ”€ Conv2d (10â†’16) + BatchNorm + Dropout
â””â”€ Conv2d (16â†’16) + BatchNorm + Dropout
    â†“
Output: Conv2d (16â†’10) + GAP
    â†“
Log Softmax (10 classes)
```

### Layer-wise Parameters

| Layer | Output Shape | Parameters |
|-------|-------------|-----------|
| Conv2d-1 | 10Ã—28Ã—28 | 100 |
| Conv2d-4 | 16Ã—28Ã—28 | 1,456 |
| Conv2d-8 | 10Ã—14Ã—14 | 170 |
| Conv2d-9 | 16Ã—14Ã—14 | 1,456 |
| Conv2d-12 | 16Ã—14Ã—14 | 2,320 |
| Conv2d-16 | 10Ã—7Ã—7 | 170 |
| Conv2d-17 | 16Ã—7Ã—7 | 1,456 |
| Conv2d-20 | 16Ã—7Ã—7 | 2,320 |
| Conv2d-23 | 10Ã—7Ã—7 | 170 |
| **Total** | | **9,798** |

## ðŸš€ Training Configuration

### Hyperparameters

- **Optimizer:** SGD with momentum (0.9)
- **Learning Rate:** 0.015 (max)
- **Weight Decay:** 0.0001
- **Batch Size:** 128
- **Epochs:** 19
- **Dropout:** 0.05
- **Scheduler:** OneCycleLR with warmup

### Data Augmentation

**Training:**
- Random Rotation: Â±7 degrees
- Normalization: mean=0.1307, std=0.3081

**Testing:**
- Standard Normalization only

## ðŸ“ˆ Training Results

### Accuracy Progression

| Epoch | Train Acc | Test Acc | Test Loss |
|-------|-----------|----------|-----------|
| 1 | 55.80% | 84.70% | 0.6716 |
| 2 | 93.90% | 97.00% | 0.1255 |
| 3 | 96.83% | 97.48% | 0.0918 |
| 4 | 97.62% | 98.32% | 0.0560 |
| 5 | 97.97% | 98.11% | 0.0608 |
| 6 | 98.20% | 98.86% | 0.0426 |
| 7 | 98.39% | **98.98%** | 0.0354 |
| 10 | 98.64% | **99.01%** | 0.0319 |
| 11 | 98.77% | **99.12%** | 0.0267 |
| 12 | 98.83% | **99.18%** | 0.0259 |
| 15 | 99.03% | **99.29%** | 0.0225 |
| **16** | **99.10%** | **99.38%** â­ | **0.0211** |
| 19 | 99.18% | 99.33% | 0.0197 |

### Performance Graph

```
Test Accuracy Over Epochs
100% â”¤                                    â•­â”€â”€â•®
 99% â”¤                          â•­â”€â”€â”€â”€â•®â”€â”€â”€â•¯  â•°â”€
 98% â”¤                â•­â”€â”€â”€â”€â•®â”€â”€â”€â”€â•¯    
 97% â”¤        â•­â”€â”€â”€â”€â”€â”€â”€â•¯              
 96% â”¤    â•­â”€â”€â”€â•¯                      
 95% â”¤    â”‚                          
 90% â”¤    â”‚                          
 85% â”¤â•­â”€â”€â”€â•¯                          
     â””â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´
      1   3   5   7   9  11  13  15  17  19
```

## ðŸ”‘ Key Features

### Efficient Design
- **1Ã—1 Convolutions:** Reduce channels in transition blocks
- **Global Average Pooling:** Eliminates fully connected layers
- **Batch Normalization:** Stabilizes training and enables higher learning rates
- **Strategic Dropout:** Prevents overfitting (5%)

### Training Optimizations
- **OneCycleLR Scheduler:** Fast convergence with learning rate warmup
- **Data Augmentation:** Improves generalization with rotation
- **SGD with Momentum:** Efficient optimization

## ðŸ’» Requirements

```bash
torch
torchvision
tqdm
torchsummary (optional)
```

## ðŸƒ Quick Start

```bash
# Install dependencies
pip install torch torchvision tqdm torchsummary

# Run training
python mnist_cnn.py
```

## ðŸ“ Model Summary

```
Total params: 9,798
Trainable params: 9,798
Non-trainable params: 0
Input size (MB): 0.00
Forward/backward pass size (MB): 0.70
Params size (MB): 0.04
Estimated Total Size (MB): 0.74
```

## ðŸŽ“ Results Analysis

### Strengths
âœ… Achieves 99.38% accuracy with minimal parameters  
âœ… Fast training (~21s per epoch on GPU)  
âœ… Efficient architecture with GAP  
âœ… Stable convergence with OneCycleLR  
âœ… Good generalization (low overfitting)

### Observations
- Model reaches 99%+ accuracy by epoch 10
- Best performance at epoch 16 (99.38%)
- Minimal gap between train and test accuracy
- Consistent performance in later epochs (99.28-99.38%)

## ðŸ“„ License

MIT License

## ðŸ¤ Contributing

Contributions, issues, and feature requests are welcome!

---

**Note:** This model demonstrates that efficient CNN architectures with proper regularization and training strategies can achieve excellent results on MNIST with minimal computational resources.
